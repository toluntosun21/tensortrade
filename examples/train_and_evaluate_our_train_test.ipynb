{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Install TensorTrade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#!python -m pip install -e .. -U"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Data Fetching"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import ssl\n",
    "import pandas as pd\n",
    "\n",
    "from tensortrade.utils import CryptoDataDownload\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Only used if pandas gives a SSLError\n",
    "\n",
    "cdd = CryptoDataDownload()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    cdd.fetch(\"Coinbase\", \"USD\", \"BTC\", \"1h\").add_prefix(\"BTC:\")#,\n",
    "], axis=1)\n",
    "data = data.rename({\"BTC:date\": \"date\"}, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                 date  BTC:open  BTC:high  BTC:low  BTC:close  BTC:volume\n0 2017-07-01 11:00:00   2505.56   2513.38  2495.12    2509.17   287000.32\n1 2017-07-01 12:00:00   2509.17   2512.87  2484.99    2488.43   393142.50\n2 2017-07-01 13:00:00   2488.43   2488.43  2454.40    2454.43   693254.01\n3 2017-07-01 14:00:00   2454.43   2473.93  2450.83    2459.35   712864.80\n4 2017-07-01 15:00:00   2459.35   2475.00  2450.00    2467.83   682105.41",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>BTC:open</th>\n      <th>BTC:high</th>\n      <th>BTC:low</th>\n      <th>BTC:close</th>\n      <th>BTC:volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-07-01 11:00:00</td>\n      <td>2505.56</td>\n      <td>2513.38</td>\n      <td>2495.12</td>\n      <td>2509.17</td>\n      <td>287000.32</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-07-01 12:00:00</td>\n      <td>2509.17</td>\n      <td>2512.87</td>\n      <td>2484.99</td>\n      <td>2488.43</td>\n      <td>393142.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-07-01 13:00:00</td>\n      <td>2488.43</td>\n      <td>2488.43</td>\n      <td>2454.40</td>\n      <td>2454.43</td>\n      <td>693254.01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-07-01 14:00:00</td>\n      <td>2454.43</td>\n      <td>2473.93</td>\n      <td>2450.83</td>\n      <td>2459.35</td>\n      <td>712864.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-07-01 15:00:00</td>\n      <td>2459.35</td>\n      <td>2475.00</td>\n      <td>2450.00</td>\n      <td>2467.83</td>\n      <td>682105.41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create features with the data module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tensortrade.data import Node, Module, DataFeed, Stream, Select\n",
    "\n",
    "\n",
    "def rsi(price: Node, period: float):\n",
    "    r = price.diff()\n",
    "    upside = r.clamp_min(0).abs()\n",
    "    downside = r.clamp_max(0).abs()\n",
    "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
    "    return 100*(1 - (1 + rs) ** -1)\n",
    "\n",
    "\n",
    "def macd(price: Node, fast: float, slow: float, signal: float) -> Node:\n",
    "    fm = price.ewm(span=fast, adjust=False).mean()\n",
    "    sm = price.ewm(span=slow, adjust=False).mean()\n",
    "    md = fm - sm\n",
    "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
    "    return signal\n",
    "\n",
    "\n",
    "features = []\n",
    "for c in data.columns[1:]:\n",
    "    s = Stream(list(data[c])).rename(data[c].name)\n",
    "    features += [s]\n",
    "\n",
    "btc_close = Select(\"BTC:close\")(*features)\n",
    "\n",
    "features += [\n",
    "    rsi(btc_close, period=24).rename(\"BTC:rsi\"),\n",
    "    macd(btc_close, fast=10, slow=50, signal=5).rename(\"BTC:macd\"),\n",
    "]\n",
    "\n",
    "feed = DataFeed(features)\n",
    "feed.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suuser\\PycharmProjects\\tensortrade\\tensortrade\\data\\stream\\node.py:933: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  v = (w[::-1] * x).sum() / w.sum()\n"
     ]
    }
   ],
   "source": [
    "feed.next()\n",
    "feed.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Trading Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tensortrade.exchanges import Exchange\n",
    "from tensortrade.exchanges.services.execution.simulated import execute_order\n",
    "from tensortrade.data import Stream, DataFeed, Module\n",
    "from tensortrade.instruments import USD, BTC, ETH\n",
    "from tensortrade.wallets import Wallet, Portfolio\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.rewards import RiskAdjustedReturns\n",
    "import copy\n",
    "\n",
    "coinbase = Exchange(\"coinbase\", service=execute_order)(\n",
    "    Stream(list(data[\"BTC:close\"])).rename(\"USD-BTC\")#,\n",
    "#    Stream(list(data[\"ETH:close\"])).rename(\"USD-ETH\")\n",
    ")\n",
    "\n",
    "portfolio = Portfolio(USD, [\n",
    "    Wallet(coinbase, 10000 * USD),\n",
    "    Wallet(coinbase, 0 * BTC),\n",
    "])\n",
    "\n",
    "env = TradingEnvironment(\n",
    "    feed=feed,\n",
    "    portfolio=portfolio,\n",
    "    use_internal=False,\n",
    "    action_scheme=\"simple\",\n",
    "    reward_scheme=RiskAdjustedReturns(return_algorithm = 'sortino', window_size = 24),\n",
    "    window_size=24\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example Data Feed Observation\n",
    "\n",
    "Even though this observation contains data from the internal data feed, since `use_internal=False` this data will not be provided as input to the observation history. The data that will be added to observation history of the environment will strictly be the nodes that have been included into the data feed that has been provided as a parameter to the trading environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'BTC:open': 2505.56,\n 'BTC:high': 2513.38,\n 'BTC:low': 2495.12,\n 'BTC:close': 2509.17,\n 'BTC:volume': 287000.32,\n 'BTC:rsi': nan,\n 'BTC:macd': -1.0105496686365867e-13,\n 'coinbase:/USD-BTC': 2509.17,\n 'coinbase:/USD:/free': 10000.0,\n 'coinbase:/USD:/locked': 0.0,\n 'coinbase:/USD:/total': 10000.0,\n 'coinbase:/BTC:/free': 0.0,\n 'coinbase:/BTC:/locked': 0.0,\n 'coinbase:/BTC:/total': 0.0,\n 'coinbase:/BTC:/worth': 0.0,\n 'net_worth': 10000.0}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.feed.next()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from tensortrade.agents import Agent, ReplayMemory\n",
    "\n",
    "A2CTransition = namedtuple('A2CTransition', ['state', 'action', 'reward', 'done', 'value'])\n",
    "\n",
    "\n",
    "class A2C_LSTMAgent_DEV(Agent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env: 'TradingEnvironment',\n",
    "                 test_feed: DataFeed = None,\n",
    "                 shared_network: tf.keras.Model = None,\n",
    "                 actor_network: tf.keras.Model = None,\n",
    "                 critic_network: tf.keras.Model = None):\n",
    "        self.env = env\n",
    "        self.test_feed = test_feed or env.feed\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.observation_shape = env.observation_space.shape\n",
    "\n",
    "        self.shared_network = shared_network or self._build_shared_network()\n",
    "        self.actor_network = actor_network or self._build_actor_network()\n",
    "        self.critic_network = critic_network or self._build_critic_network()\n",
    "\n",
    "        self.env.agent_id = self.id\n",
    "\n",
    "    def _build_shared_network(self):\n",
    "        self.LSTM = tf.keras.layers.LSTM(50, return_sequences=True, stateful=True)\n",
    "\n",
    "        network = tf.keras.Sequential([\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.InputLayer(input_shape=self.observation_shape)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(pool_size=2)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(pool_size=2)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()),\n",
    "            self.LSTM\n",
    "\n",
    "        ])\n",
    "        return network\n",
    "\n",
    "    def _build_actor_network(self):\n",
    "        actor_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.n_actions, activation='relu')\n",
    "        ])\n",
    "        return tf.keras.Sequential([self.shared_network, actor_head])\n",
    "\n",
    "    def _build_critic_network(self):\n",
    "        critic_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        return tf.keras.Sequential([self.shared_network, critic_head])\n",
    "\n",
    "    def restore(self, path: str, **kwargs):\n",
    "        actor_filename: str = kwargs.get('actor_filename', None)\n",
    "        critic_filename: str = kwargs.get('critic_filename', None)\n",
    "\n",
    "        if not actor_filename or not critic_filename:\n",
    "            raise ValueError(\n",
    "                'The `restore` method requires a directory `path`, a `critic_filename`, and an `actor_filename`.')\n",
    "\n",
    "        self.actor_network = tf.keras.models.load_model(path + actor_filename)\n",
    "        self.critic_network = tf.keras.models.load_model(path + critic_filename)\n",
    "\n",
    "    def save(self, path: str, **kwargs):\n",
    "        episode: int = kwargs.get('episode', None)\n",
    "\n",
    "        if episode:\n",
    "            suffix = self.id + \"__\" + str(episode).zfill(3) + \".hdf5\"\n",
    "            actor_filename = \"actor_network__\" + suffix\n",
    "            critic_filename = \"critic_network__\" + suffix\n",
    "        else:\n",
    "            actor_filename = \"actor_network__\" + self.id + \".hdf5\"\n",
    "            critic_filename = \"critic_network__\" + self.id + \".hdf5\"\n",
    "\n",
    "        self.actor_network.save(path + actor_filename)\n",
    "        self.critic_network.save(path + critic_filename)\n",
    "\n",
    "    def get_action(self, state: np.ndarray, **kwargs) -> int:\n",
    "        threshold: float = kwargs.get('threshold', 0)\n",
    "\n",
    "        rand = random.random()\n",
    "\n",
    "        if rand < threshold:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            logits = self.actor_network(state[None, None, :], training=False)\n",
    "            return tf.squeeze(tf.squeeze(tf.random.categorical(logits[0], 1), axis=-1), axis=-1)\n",
    "\n",
    "    def _apply_gradient_descent(self,\n",
    "                                memory: ReplayMemory,\n",
    "                                batch_size: int,\n",
    "                                learning_rate: float,\n",
    "                                discount_factor: float,\n",
    "                                entropy_c: float,):\n",
    "\n",
    "        if hasattr(self, 'trained_lstm_states'):\n",
    "            self.LSTM.states = copy.deepcopy(self.trained_lstm_states)\n",
    "        else:\n",
    "            self.trained_lstm_states = copy.deepcopy(self.LSTM.states)\n",
    "            self.LSTM.reset_states()\n",
    "\n",
    "        huber_loss = tf.keras.losses.Huber()\n",
    "        wsce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "        transitions = memory.tail(batch_size)\n",
    "        batch = A2CTransition(*zip(*transitions))\n",
    "\n",
    "        states = tf.convert_to_tensor(batch.state)\n",
    "        actions = tf.convert_to_tensor(batch.action)\n",
    "        rewards = tf.convert_to_tensor(batch.reward, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(batch.done)\n",
    "        values = tf.convert_to_tensor(batch.value)\n",
    "\n",
    "        returns = []\n",
    "        exp_weighted_return = 0\n",
    "\n",
    "        for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "            exp_weighted_return = reward + discount_factor * exp_weighted_return * (1 - int(done))\n",
    "            returns += [exp_weighted_return]\n",
    "\n",
    "        returns = returns[::-1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            state_values = self.critic_network(states[None,:])\n",
    "            critic_loss_value = huber_loss(returns, state_values)\n",
    "\n",
    "        gradients = tape.gradient(critic_loss_value, self.critic_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.critic_network.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            returns = tf.reshape(returns, [batch_size, 1])\n",
    "            advantages = returns - values\n",
    "\n",
    "            actions = tf.cast(actions, tf.int32)\n",
    "            logits = self.actor_network(states[None,:])\n",
    "            policy_loss_value = wsce_loss(actions, logits, sample_weight=advantages)\n",
    "\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            entropy_loss_value = tf.keras.losses.categorical_crossentropy(probs, probs)\n",
    "            policy_total_loss_value = policy_loss_value - entropy_c * entropy_loss_value\n",
    "\n",
    "        gradients = tape.gradient(policy_total_loss_value,\n",
    "                                  self.actor_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.actor_network.trainable_variables))\n",
    "        self.trained_lstm_states = copy.deepcopy(self.LSTM.states)\n",
    "\n",
    "    def train(self,\n",
    "              n_steps: int = None,\n",
    "              n_episodes: int = None,\n",
    "              save_every: int = None,\n",
    "              save_path: str = None,\n",
    "              callback: callable = None,\n",
    "              **kwargs) -> float:\n",
    "        batch_size: int = kwargs.get('batch_size', 128)\n",
    "        discount_factor: float = kwargs.get('discount_factor', 0.9999)\n",
    "        learning_rate: float = kwargs.get('learning_rate', 0.0001)\n",
    "        eps_start: float = kwargs.get('eps_start', 0.9)\n",
    "        eps_end: float = kwargs.get('eps_end', 0.05)\n",
    "        eps_decay_steps: int = kwargs.get('eps_decay_steps', 400)\n",
    "        entropy_c: int = kwargs.get('entropy_c', 0.0001)\n",
    "        memory_capacity: int = kwargs.get('memory_capacity', 1000)\n",
    "        train_end: float = kwargs.get('train_end', 0.3)\n",
    "\n",
    "        memory = ReplayMemory(memory_capacity, transition_type=A2CTransition)\n",
    "        episode = 0\n",
    "        steps_done = 0\n",
    "        total_reward = 0\n",
    "        stop_training = False\n",
    "\n",
    "        if n_steps and not n_episodes:\n",
    "            n_episodes = np.iinfo(np.int32).max\n",
    "\n",
    "        print('====      AGENT ID: {}      ===='.format(self.id))\n",
    "\n",
    "        while episode < n_episodes and not stop_training:\n",
    "            if not episode or not self.env.feed.has_next():\n",
    "                state = self.env.reset()\n",
    "            done = False\n",
    "            steps_done = 0\n",
    "            if episode:\n",
    "                #self.LSTM.reset_states()\n",
    "                memory = ReplayMemory(memory_capacity, transition_type=A2CTransition)\n",
    "            print(self.env.portfolio.balances)\n",
    "            print('====      TRAIN EPISODE ID ({}/{}): {}      ===='.format(episode + 1,\n",
    "                                                                      n_episodes,\n",
    "                                                                      self.env.episode_id))\n",
    "\n",
    "            while not done:\n",
    "                if steps_done % 24 == 0: #each day\n",
    "                    print(\"step {}/{}\".format(steps_done, n_steps))\n",
    "                    print(self.env.portfolio.balances)\n",
    "                    print(self.env.portfolio.net_worth)\n",
    "                    print(state[-1][0])\n",
    "\n",
    "                if not self.env.feed.has_next():\n",
    "                    done = True\n",
    "                    continue\n",
    "                threshold = eps_end + (eps_start - eps_end) * np.exp(-steps_done / eps_decay_steps)\n",
    "                action = self.get_action(state, threshold=threshold)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                value = self.critic_network(state[None, None, :], training=False)\n",
    "                value = tf.squeeze(value, axis=-1)\n",
    "\n",
    "                memory.push(state, action, reward, done, value)\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps_done += 1\n",
    "\n",
    "                if self.env.portfolio.net_worth < self.env.portfolio.initial_net_worth * train_end:\n",
    "                    done = True\n",
    "                    continue\n",
    "\n",
    "                if len(memory) < batch_size:\n",
    "                    continue\n",
    "\n",
    "                if steps_done % batch_size == 0:\n",
    "                    self._apply_gradient_descent(memory,\n",
    "                                             batch_size,\n",
    "                                             learning_rate,\n",
    "                                             discount_factor,\n",
    "                                             entropy_c)\n",
    "\n",
    "                if n_steps and steps_done >= n_steps:\n",
    "                    done = True\n",
    "                    #stop_training = True\n",
    "\n",
    "            # VALIDATION\n",
    "            state = self.env.reset()\n",
    "            train_feed = self.env.feed\n",
    "            self.env.set_feed(self.test_feed)\n",
    "            self.env.compile()\n",
    "            self.env.feed.next()\n",
    "            state = self.env.reset()\n",
    "            self.LSTM.reset_states()\n",
    "            done = False\n",
    "            steps_done = 0\n",
    "            threshold = 0\n",
    "\n",
    "            print(self.env.portfolio.balances)\n",
    "            print('====      TEST EPISODE ID ({}/{}): {}      ===='.format(episode + 1,\n",
    "                                                                      n_episodes,\n",
    "                                                                      self.env.episode_id))\n",
    "\n",
    "            while not done:\n",
    "                if steps_done % 24 == 0: #each day\n",
    "                    print(\"step {}/{}\".format(steps_done, n_steps))\n",
    "                    print(self.env.portfolio.balances)\n",
    "                    print(self.env.portfolio.net_worth)\n",
    "                    print(state[-1][0])\n",
    "\n",
    "                if not self.env.feed.has_next():\n",
    "                    done = True\n",
    "                    continue\n",
    "\n",
    "                action = self.get_action(state, threshold=threshold)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                value = self.critic_network(state[None, None, :], training=False)\n",
    "                value = tf.squeeze(value, axis=-1)\n",
    "                state = next_state\n",
    "                steps_done += 1\n",
    "                if self.env.portfolio.net_worth < self.env.portfolio.initial_net_worth * train_end:\n",
    "                    done = True\n",
    "                    continue\n",
    "\n",
    "            portfolio_perf = self.env.portfolio.performance.values\n",
    "            np.savetxt(save_path+'/test{}.csv'.format(episode+1), portfolio_perf, delimiter=',', fmt='%s')\n",
    "\n",
    "            self.env.set_feed(train_feed)\n",
    "            self.env.compile()\n",
    "            self.env.portfolio.reset()\n",
    "            #self.env.feed.next()\n",
    "            #state = self.env.reset()\n",
    "            self.LSTM.reset_states()\n",
    "\n",
    "            is_checkpoint = save_every and episode % save_every == 0\n",
    "\n",
    "            if save_path and (is_checkpoint or episode == n_episodes):\n",
    "                self.save(save_path, episode=episode)\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "        mean_reward = total_reward / steps_done\n",
    "\n",
    "        return mean_reward\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====      AGENT ID: 8dd3e75a-1482-4a47-8e60-ff7b461be4c0      ====\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TRAIN EPISODE ID (1/500): 40c4554d-03f5-4406-b33d-9f442eb5bc9f      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "10000.0\n",
      "step 24/25272\n",
      "[7732.71 USD, 0.80345077 BTC]\n",
      "9700.843006192\n",
      "step 48/25272\n",
      "[0.00 USD, 3.88767214 BTC]\n",
      "9728.121995922\n",
      "step 72/25272\n",
      "[23.25 USD, 3.69014234 BTC]\n",
      "9672.713909136199\n",
      "step 96/25272\n",
      "[9167.92 USD, 0.03235488 BTC]\n",
      "9251.1386926528\n",
      "step 120/25272\n",
      "[2754.81 USD, 2.48350054 BTC]\n",
      "9219.3122356092\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TEST EPISODE ID (1/500): 49808050-fbe7-444b-92b7-530ffde2e68a      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "10000.0\n",
      "step 24/25272\n",
      "[3290.55 USD, 2.59923239 BTC]\n",
      "9657.629662544\n",
      "step 48/25272\n",
      "[2766.80 USD, 2.74543592 BTC]\n",
      "9636.704302615999\n",
      "step 72/25272\n",
      "[1594.59 USD, 3.17970028 BTC]\n",
      "9909.2836531804\n",
      "step 96/25272\n",
      "[5342.78 USD, 1.62822374 BTC]\n",
      "9530.669152704399\n",
      "step 120/25272\n",
      "[8647.75 USD, 0.24997510 BTC]\n",
      "9298.430185798\n",
      "step 144/25272\n",
      "[9113.37 USD, 0.00000000 BTC]\n",
      "9113.37\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TRAIN EPISODE ID (2/500): 49808050-fbe7-444b-92b7-530ffde2e68a      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "None\n",
      "step 24/25272\n",
      "[0.00 USD, 4.03047109 BTC]\n",
      "9818.509708216301\n",
      "step 48/25272\n",
      "[9426.83 USD, 0.09365459 BTC]\n",
      "9659.289121293099\n",
      "step 72/25272\n",
      "[0.00 USD, 3.79497291 BTC]\n",
      "9847.3095560553\n",
      "step 96/25272\n",
      "[9547.22 USD, 0.00000000 BTC]\n",
      "9547.22\n",
      "step 120/25272\n",
      "[6845.44 USD, 1.02289228 BTC]\n",
      "9503.0778062048\n",
      "step 144/25272\n",
      "[554.89 USD, 3.37751185 BTC]\n",
      "9277.922304757998\n",
      "step 168/25272\n",
      "[942.12 USD, 3.26708373 BTC]\n",
      "9175.1709996\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TEST EPISODE ID (2/500): 916b2f10-caca-49e7-b6c8-03ea2de11be5      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "10000.0\n",
      "step 24/25272\n",
      "[73.70 USD, 3.88251753 BTC]\n",
      "9584.314941488\n",
      "step 48/25272\n",
      "[8671.61 USD, 0.31599805 BTC]\n",
      "9462.331920515\n",
      "step 72/25272\n",
      "[637.64 USD, 3.44349761 BTC]\n",
      "9642.1452053173\n",
      "step 96/25272\n",
      "[6041.68 USD, 1.31013482 BTC]\n",
      "9411.4253651292\n",
      "step 120/25272\n",
      "[1528.93 USD, 2.97775157 BTC]\n",
      "9279.9577816786\n",
      "step 144/25272\n",
      "[9052.63 USD, 0.00000000 BTC]\n",
      "9052.63\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TRAIN EPISODE ID (3/500): 916b2f10-caca-49e7-b6c8-03ea2de11be5      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "None\n",
      "step 24/25272\n",
      "[8727.08 USD, 0.36424306 BTC]\n",
      "9614.4015911742\n",
      "step 48/25272\n",
      "[18.69 USD, 3.76684299 BTC]\n",
      "9368.3333170491\n",
      "step 72/25272\n",
      "[239.56 USD, 3.54694124 BTC]\n",
      "9443.2695377892\n",
      "step 96/25272\n",
      "[8694.70 USD, 0.19291848 BTC]\n",
      "9190.5024227848\n",
      "step 120/25272\n",
      "[9114.80 USD, 0.00000000 BTC]\n",
      "9114.8\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TEST EPISODE ID (3/500): 068a06ce-a917-409b-9414-8bb5c3e92789      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "10000.0\n",
      "step 24/25272\n",
      "[8666.60 USD, 0.37424710 BTC]\n",
      "9583.35569616\n",
      "step 48/25272\n",
      "[500.21 USD, 3.66077524 BTC]\n",
      "9660.567883052\n",
      "step 72/25272\n",
      "[4255.86 USD, 2.15155990 BTC]\n",
      "9882.038529307\n",
      "step 96/25272\n",
      "[9618.64 USD, 0.00000000 BTC]\n",
      "9618.64\n",
      "step 120/25272\n",
      "[5220.87 USD, 1.65664262 BTC]\n",
      "9533.077607007599\n",
      "step 144/25272\n",
      "[272.88 USD, 3.49455775 BTC]\n",
      "9204.969608999998\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "====      TRAIN EPISODE ID (4/500): 068a06ce-a917-409b-9414-8bb5c3e92789      ====\n",
      "step 0/25272\n",
      "[10000.00 USD, 0.00000000 BTC]\n",
      "None\n",
      "step 24/25272\n",
      "[3751.12 USD, 2.47335444 BTC]\n",
      "9776.3845506508\n",
      "step 48/25272\n",
      "[0.00 USD, 3.85946657 BTC]\n",
      "9579.5433787313\n",
      "step 72/25272\n",
      "[9811.19 USD, 0.00000000 BTC]\n",
      "9811.19\n",
      "step 96/25272\n",
      "[8982.82 USD, 0.23443501 BTC]\n",
      "9585.3203200501\n",
      "step 120/25272\n",
      "[2933.75 USD, 2.54669966 BTC]\n",
      "9550.4831886256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-19-186031062dfc>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0magent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mA2C_LSTM_Agent\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_feed\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m24\u001B[0m\u001B[1;33m*\u001B[0m\u001B[1;36m7\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_end\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.9\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_episodes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m500\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"D:/Users/suuser/Desktop/test_results\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\tensortrade\\tensortrade\\agents\\a2c_lstm_agent.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, n_steps, n_episodes, save_every, save_path, callback, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m                 \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthreshold\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mthreshold\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m                 \u001B[0mnext_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m                 \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcritic_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m                 \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    966\u001B[0m           with base_layer_utils.autocast_context_manager(\n\u001B[0;32m    967\u001B[0m               self._compute_dtype):\n\u001B[1;32m--> 968\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    969\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    970\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_set_mask_metadata\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    289\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'training'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    290\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 291\u001B[1;33m       \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    292\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    966\u001B[0m           with base_layer_utils.autocast_context_manager(\n\u001B[0;32m    967\u001B[0m               self._compute_dtype):\n\u001B[1;32m--> 968\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    969\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    970\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_set_mask_metadata\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    289\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'training'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    290\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 291\u001B[1;33m       \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    292\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, initial_state, constants, **kwargs)\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    653\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0minitial_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mconstants\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 654\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mRNN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    655\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    656\u001B[0m     \u001B[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    966\u001B[0m           with base_layer_utils.autocast_context_manager(\n\u001B[0;32m    967\u001B[0m               self._compute_dtype):\n\u001B[1;32m--> 968\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    969\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    970\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_set_mask_metadata\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, mask, training, initial_state)\u001B[0m\n\u001B[0;32m   1181\u001B[0m               **gpu_lstm_kwargs)\n\u001B[0;32m   1182\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1183\u001B[1;33m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001B[0m\u001B[0;32m   1184\u001B[0m               **normal_lstm_kwargs)\n\u001B[0;32m   1185\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001B[0m in \u001B[0;36mstandard_lstm\u001B[1;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, activation, recurrent_activation, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001B[0m\n\u001B[0;32m   1308\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1309\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1310\u001B[1;33m   last_output, outputs, new_states = K.rnn(\n\u001B[0m\u001B[0;32m   1311\u001B[0m       \u001B[0mstep\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1312\u001B[0m       \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0minit_h\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minit_c\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001B[0m in \u001B[0;36mrnn\u001B[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001B[0m\n\u001B[0;32m   4089\u001B[0m     \u001B[1;31m# output_time_zero is used to determine the cell output shape and its dtype.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4090\u001B[0m     \u001B[1;31m# the value is discarded.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4091\u001B[1;33m     output_time_zero, _ = step_function(\n\u001B[0m\u001B[0;32m   4092\u001B[0m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001B[0;32m   4093\u001B[0m     output_ta = tuple(\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(cell_inputs, cell_states)\u001B[0m\n\u001B[0;32m   1298\u001B[0m     \u001B[0mz\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias_add\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1299\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1300\u001B[1;33m     \u001B[0mz0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1301\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1302\u001B[0m     \u001B[0mi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrecurrent_activation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36msplit\u001B[1;34m(value, num_or_size_splits, axis, num, name)\u001B[0m\n\u001B[0;32m   1944\u001B[0m   if isinstance(num_or_size_splits,\n\u001B[0;32m   1945\u001B[0m                 (numbers.Integral, tensor_shape.Dimension)):\n\u001B[1;32m-> 1946\u001B[1;33m     return gen_array_ops.split(\n\u001B[0m\u001B[0;32m   1947\u001B[0m         axis=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001B[0;32m   1948\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\suuser\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36msplit\u001B[1;34m(axis, value, num_split, name)\u001B[0m\n\u001B[0;32m   9706\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9707\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 9708\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   9709\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Split\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9710\u001B[0m         tld.op_callbacks, axis, value, \"num_split\", num_split)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import tensortrade.agents.a2c_lstm_agent\n",
    "from tensortrade.agents.a2c_lstm_agent import A2C_LSTM_Agent\n",
    "import importlib\n",
    "importlib.reload(tensortrade.agents.a2c_lstm_agent)\n",
    "\n",
    "agent = A2C_LSTM_Agent(env, validation_feed=copy.deepcopy(feed))\n",
    "\n",
    "agent.train(n_steps=data.shape[0], batch_size=24*7, train_end=0.3, n_episodes=500, save_path=\"D:/Users/suuser/Desktop/test_results\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}