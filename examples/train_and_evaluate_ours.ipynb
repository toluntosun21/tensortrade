{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install -e .. -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import pandas as pd\n",
    "\n",
    "from tensortrade.utils import CryptoDataDownload\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Only used if pandas gives a SSLError\n",
    "\n",
    "cdd = CryptoDataDownload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    cdd.fetch(\"Coinbase\", \"USD\", \"BTC\", \"1h\").add_prefix(\"BTC:\")#,\n",
    "    #cdd.fetch(\"Coinbase\", \"USD\", \"ETH\", \"1h\").add_prefix(\"ETH:\")\n",
    "], axis=1)\n",
    "#data = data.drop([\"ETH:date\"], axis=1)\n",
    "data = data.rename({\"BTC:date\": \"date\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                 date  BTC:open  BTC:high  BTC:low  BTC:close  BTC:volume  \\\n0 2017-07-01 11:00:00   2505.56   2513.38  2495.12    2509.17   287000.32   \n1 2017-07-01 12:00:00   2509.17   2512.87  2484.99    2488.43   393142.50   \n2 2017-07-01 13:00:00   2488.43   2488.43  2454.40    2454.43   693254.01   \n3 2017-07-01 14:00:00   2454.43   2473.93  2450.83    2459.35   712864.80   \n4 2017-07-01 15:00:00   2459.35   2475.00  2450.00    2467.83   682105.41   \n\n   ETH:open  ETH:high  ETH:low  ETH:close  ETH:volume  \n0    279.98    279.99    272.1     275.01   679358.87  \n1    275.01    275.01    271.0     274.83   824362.87  \n2    274.83    274.93    265.0     268.79  3010787.99  \n3    268.79    269.90    265.0     265.74  1702536.85  \n4    265.74    272.74    265.0     272.57  1500282.55  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>BTC:open</th>\n      <th>BTC:high</th>\n      <th>BTC:low</th>\n      <th>BTC:close</th>\n      <th>BTC:volume</th>\n      <th>ETH:open</th>\n      <th>ETH:high</th>\n      <th>ETH:low</th>\n      <th>ETH:close</th>\n      <th>ETH:volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-07-01 11:00:00</td>\n      <td>2505.56</td>\n      <td>2513.38</td>\n      <td>2495.12</td>\n      <td>2509.17</td>\n      <td>287000.32</td>\n      <td>279.98</td>\n      <td>279.99</td>\n      <td>272.1</td>\n      <td>275.01</td>\n      <td>679358.87</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-07-01 12:00:00</td>\n      <td>2509.17</td>\n      <td>2512.87</td>\n      <td>2484.99</td>\n      <td>2488.43</td>\n      <td>393142.50</td>\n      <td>275.01</td>\n      <td>275.01</td>\n      <td>271.0</td>\n      <td>274.83</td>\n      <td>824362.87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-07-01 13:00:00</td>\n      <td>2488.43</td>\n      <td>2488.43</td>\n      <td>2454.40</td>\n      <td>2454.43</td>\n      <td>693254.01</td>\n      <td>274.83</td>\n      <td>274.93</td>\n      <td>265.0</td>\n      <td>268.79</td>\n      <td>3010787.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-07-01 14:00:00</td>\n      <td>2454.43</td>\n      <td>2473.93</td>\n      <td>2450.83</td>\n      <td>2459.35</td>\n      <td>712864.80</td>\n      <td>268.79</td>\n      <td>269.90</td>\n      <td>265.0</td>\n      <td>265.74</td>\n      <td>1702536.85</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-07-01 15:00:00</td>\n      <td>2459.35</td>\n      <td>2475.00</td>\n      <td>2450.00</td>\n      <td>2467.83</td>\n      <td>682105.41</td>\n      <td>265.74</td>\n      <td>272.74</td>\n      <td>265.0</td>\n      <td>272.57</td>\n      <td>1500282.55</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features with the data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.data import Node, Module, DataFeed, Stream, Select\n",
    "\n",
    "\n",
    "def rsi(price: Node, period: float):\n",
    "    r = price.diff()\n",
    "    upside = r.clamp_min(0).abs()\n",
    "    downside = r.clamp_max(0).abs()\n",
    "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
    "    return 100*(1 - (1 + rs) ** -1)\n",
    "\n",
    "\n",
    "def macd(price: Node, fast: float, slow: float, signal: float) -> Node:\n",
    "    fm = price.ewm(span=fast, adjust=False).mean()\n",
    "    sm = price.ewm(span=slow, adjust=False).mean()\n",
    "    md = fm - sm\n",
    "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
    "    return signal\n",
    "\n",
    "\n",
    "features = []\n",
    "for c in data.columns[1:]:\n",
    "    s = Stream(list(data[c])).rename(data[c].name)\n",
    "    features += [s]\n",
    "\n",
    "btc_close = Select(\"BTC:close\")(*features)\n",
    "#eth_close = Select(\"ETH:close\")(*features)\n",
    "\n",
    "features += [\n",
    "    rsi(btc_close, period=24).rename(\"BTC:rsi\"),\n",
    "    macd(btc_close, fast=10, slow=50, signal=5).rename(\"BTC:macd\"),\n",
    "    #rsi(eth_close, period=20).rename(\"ETH:rsi\"),\n",
    "    #macd(eth_close, fast=10, slow=50, signal=5).rename(\"ETH:macd\")\n",
    "]\n",
    "\n",
    "feed = DataFeed(features)\n",
    "feed.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suuser\\PycharmProjects\\tensortrade\\tensortrade\\data\\stream\\node.py:933: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  v = (w[::-1] * x).sum() / w.sum()\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'BTC:open': 2505.56,\n 'BTC:high': 2513.38,\n 'BTC:low': 2495.12,\n 'BTC:close': 2509.17,\n 'BTC:volume': 287000.32,\n 'ETH:open': 279.98,\n 'ETH:high': 279.99,\n 'ETH:low': 272.1,\n 'ETH:close': 275.01,\n 'ETH:volume': 679358.87,\n 'BTC:rsi': nan,\n 'BTC:macd': 0.0,\n 'ETH:rsi': nan,\n 'ETH:macd': 0.0}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed.next()\n",
    "feed.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.exchanges import Exchange\n",
    "from tensortrade.exchanges.services.execution.simulated import execute_order\n",
    "from tensortrade.data import Stream, DataFeed, Module\n",
    "from tensortrade.instruments import USD, BTC, ETH\n",
    "from tensortrade.wallets import Wallet, Portfolio\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.rewards import RiskAdjustedReturns\n",
    "\n",
    "coinbase = Exchange(\"coinbase\", service=execute_order)(\n",
    "    Stream(list(data[\"BTC:close\"])).rename(\"USD-BTC\")#,\n",
    "#    Stream(list(data[\"ETH:close\"])).rename(\"USD-ETH\")\n",
    ")\n",
    "\n",
    "portfolio = Portfolio(USD, [\n",
    "    Wallet(coinbase, 10000 * USD),\n",
    "    Wallet(coinbase, 0 * BTC),\n",
    "])\n",
    "\n",
    "env = TradingEnvironment(\n",
    "    feed=feed,\n",
    "    portfolio=portfolio,\n",
    "    use_internal=False,\n",
    "    action_scheme=\"simple\",\n",
    "    reward_scheme=RiskAdjustedReturns(return_algorithm = 'sortino', window_size = 24),\n",
    "    window_size=24\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data Feed Observation\n",
    "\n",
    "Even though this observation contains data from the internal data feed, since `use_internal=False` this data will not be provided as input to the observation history. The data that will be added to observation history of the environment will strictly be the nodes that have been included into the data feed that has been provided as a parameter to the trading environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'BTC:open': 2505.56,\n 'BTC:high': 2513.38,\n 'BTC:low': 2495.12,\n 'BTC:close': 2509.17,\n 'BTC:volume': 287000.32,\n 'ETH:open': 279.98,\n 'ETH:high': 279.99,\n 'ETH:low': 272.1,\n 'ETH:close': 275.01,\n 'ETH:volume': 679358.87,\n 'BTC:rsi': 0.0,\n 'BTC:macd': -0.23222985476439617,\n 'ETH:rsi': 0.0,\n 'ETH:macd': -0.0020154953644232945,\n 'coinbase:/USD-BTC': 2509.17,\n 'coinbase:/USD-ETH': 275.01,\n 'coinbase:/USD:/free': 10000.0,\n 'coinbase:/USD:/locked': 0.0,\n 'coinbase:/USD:/total': 10000.0,\n 'coinbase:/BTC:/free': 0.0,\n 'coinbase:/BTC:/locked': 0.0,\n 'coinbase:/BTC:/total': 0.0,\n 'coinbase:/BTC:/worth': 0.0,\n 'coinbase:/ETH:/free': 0.0,\n 'coinbase:/ETH:/locked': 0.0,\n 'coinbase:/ETH:/total': 0.0,\n 'coinbase:/ETH:/worth': 0.0,\n 'net_worth': 10000.0}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.feed.next()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from tensortrade.agents import Agent, ReplayMemory\n",
    "\n",
    "A2CTransition = namedtuple('A2CTransition', ['state', 'action', 'reward', 'done', 'value'])\n",
    "\n",
    "\n",
    "class A2C_LSTMAgent_DEV(Agent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env: 'TradingEnvironment',\n",
    "                 shared_network: tf.keras.Model = None,\n",
    "                 actor_network: tf.keras.Model = None,\n",
    "                 critic_network: tf.keras.Model = None):\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.observation_shape = env.observation_space.shape\n",
    "\n",
    "        self.shared_network = shared_network or self._build_shared_network()\n",
    "        self.actor_network = actor_network or self._build_actor_network()\n",
    "        self.critic_network = critic_network or self._build_critic_network()\n",
    "\n",
    "        self.env.agent_id = self.id\n",
    "\n",
    "    def _build_shared_network(self):\n",
    "        self.LSTM = tf.keras.layers.LSTM(50, return_sequences=True, stateful=True)\n",
    "\n",
    "        network = tf.keras.Sequential([\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.InputLayer(input_shape=self.observation_shape)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(pool_size=2)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(pool_size=2)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()),\n",
    "            self.LSTM\n",
    "\n",
    "        ])\n",
    "        return network\n",
    "\n",
    "    def _build_actor_network(self):\n",
    "        actor_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.n_actions, activation='relu')\n",
    "        ])\n",
    "        return tf.keras.Sequential([self.shared_network, actor_head])\n",
    "\n",
    "    def _build_critic_network(self):\n",
    "        critic_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        return tf.keras.Sequential([self.shared_network, critic_head])\n",
    "\n",
    "    def restore(self, path: str, **kwargs):\n",
    "        actor_filename: str = kwargs.get('actor_filename', None)\n",
    "        critic_filename: str = kwargs.get('critic_filename', None)\n",
    "\n",
    "        if not actor_filename or not critic_filename:\n",
    "            raise ValueError(\n",
    "                'The `restore` method requires a directory `path`, a `critic_filename`, and an `actor_filename`.')\n",
    "\n",
    "        self.actor_network = tf.keras.models.load_model(path + actor_filename)\n",
    "        self.critic_network = tf.keras.models.load_model(path + critic_filename)\n",
    "\n",
    "    def save(self, path: str, **kwargs):\n",
    "        episode: int = kwargs.get('episode', None)\n",
    "\n",
    "        if episode:\n",
    "            suffix = self.id + \"__\" + str(episode).zfill(3) + \".hdf5\"\n",
    "            actor_filename = \"actor_network__\" + suffix\n",
    "            critic_filename = \"critic_network__\" + suffix\n",
    "        else:\n",
    "            actor_filename = \"actor_network__\" + self.id + \".hdf5\"\n",
    "            critic_filename = \"critic_network__\" + self.id + \".hdf5\"\n",
    "\n",
    "        self.actor_network.save(path + actor_filename)\n",
    "        self.critic_network.save(path + critic_filename)\n",
    "\n",
    "    def get_action(self, state: np.ndarray, **kwargs) -> int:\n",
    "        threshold: float = kwargs.get('threshold', 0)\n",
    "\n",
    "        rand = random.random()\n",
    "\n",
    "        if rand < threshold:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            logits = self.actor_network(state[None, None, :], training=False)\n",
    "            return tf.squeeze(tf.squeeze(tf.random.categorical(logits[0], 1), axis=-1), axis=-1)\n",
    "\n",
    "    def _apply_gradient_descent(self,\n",
    "                                memory: ReplayMemory,\n",
    "                                batch_size: int,\n",
    "                                learning_rate: float,\n",
    "                                discount_factor: float,\n",
    "                                entropy_c: float,):\n",
    "\n",
    "        if hasattr(self, 'trained_lstm_states'):\n",
    "            self.LSTM.states = copy.deepcopy(self.trained_lstm_states)\n",
    "        else:\n",
    "            self.trained_lstm_states = copy.deepcopy(self.LSTM.states)\n",
    "            self.LSTM.reset_states()\n",
    "\n",
    "        huber_loss = tf.keras.losses.Huber()\n",
    "        wsce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "        transitions = memory.tail(batch_size)\n",
    "        batch = A2CTransition(*zip(*transitions))\n",
    "\n",
    "        states = tf.convert_to_tensor(batch.state)\n",
    "        actions = tf.convert_to_tensor(batch.action)\n",
    "        rewards = tf.convert_to_tensor(batch.reward, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(batch.done)\n",
    "        values = tf.convert_to_tensor(batch.value)\n",
    "\n",
    "        returns = []\n",
    "        exp_weighted_return = 0\n",
    "\n",
    "        for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "            exp_weighted_return = reward + discount_factor * exp_weighted_return * (1 - int(done))\n",
    "            returns += [exp_weighted_return]\n",
    "\n",
    "        returns = returns[::-1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            state_values = self.critic_network(states[None,:])\n",
    "            critic_loss_value = huber_loss(returns, state_values)\n",
    "\n",
    "        gradients = tape.gradient(critic_loss_value, self.critic_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.critic_network.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            returns = tf.reshape(returns, [batch_size, 1])\n",
    "            advantages = returns - values\n",
    "\n",
    "            actions = tf.cast(actions, tf.int32)\n",
    "            logits = self.actor_network(states[None,:])\n",
    "            policy_loss_value = wsce_loss(actions, logits, sample_weight=advantages)\n",
    "\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            entropy_loss_value = tf.keras.losses.categorical_crossentropy(probs, probs)\n",
    "            policy_total_loss_value = policy_loss_value - entropy_c * entropy_loss_value\n",
    "\n",
    "        gradients = tape.gradient(policy_total_loss_value,\n",
    "                                  self.actor_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.actor_network.trainable_variables))\n",
    "        self.trained_lstm_states = copy.deepcopy(self.LSTM.states)\n",
    "\n",
    "    def train(self,\n",
    "              n_steps: int = None,\n",
    "              n_episodes: int = None,\n",
    "              save_every: int = None,\n",
    "              save_path: str = None,\n",
    "              callback: callable = None,\n",
    "              **kwargs) -> float:\n",
    "        batch_size: int = kwargs.get('batch_size', 128)\n",
    "        discount_factor: float = kwargs.get('discount_factor', 0.9999)\n",
    "        learning_rate: float = kwargs.get('learning_rate', 0.0001)\n",
    "        eps_start: float = kwargs.get('eps_start', 0.9)\n",
    "        eps_end: float = kwargs.get('eps_end', 0.05)\n",
    "        eps_decay_steps: int = kwargs.get('eps_decay_steps', 200)\n",
    "        entropy_c: int = kwargs.get('entropy_c', 0.0001)\n",
    "        memory_capacity: int = kwargs.get('memory_capacity', 1000)\n",
    "\n",
    "        memory = ReplayMemory(memory_capacity, transition_type=A2CTransition)\n",
    "        episode = 0\n",
    "        steps_done = 0\n",
    "        total_reward = 0\n",
    "        stop_training = False\n",
    "\n",
    "        if n_steps and not n_episodes:\n",
    "            n_episodes = np.iinfo(np.int32).max\n",
    "\n",
    "        print('====      AGENT ID: {}      ===='.format(self.id))\n",
    "\n",
    "        while episode < n_episodes and not stop_training:\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps_done = 0\n",
    "            if episode:\n",
    "                self.LSTM.reset_states()\n",
    "                memory = ReplayMemory(memory_capacity, transition_type=A2CTransition)\n",
    "            print(self.env.portfolio.balances)\n",
    "            print('====      EPISODE ID ({}/{}): {}      ===='.format(episode + 1,\n",
    "                                                                      n_episodes,\n",
    "                                                                      self.env.episode_id))\n",
    "\n",
    "            while not done:\n",
    "                if steps_done % 24 == 0: #each day\n",
    "                    print(\"step {}/{}\".format(steps_done, n_steps))\n",
    "                    print(self.env.portfolio.balances)\n",
    "                    print(self.env.portfolio.net_worth)\n",
    "\n",
    "                if not self.env.feed.has_next():\n",
    "                    done = True\n",
    "                    continue\n",
    "\n",
    "                threshold = eps_end + (eps_start - eps_end) * np.exp(-steps_done / eps_decay_steps)\n",
    "                action = self.get_action(state, threshold=threshold)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                value = self.critic_network(state[None, None, :], training=False)\n",
    "                value = tf.squeeze(value, axis=-1)\n",
    "\n",
    "                memory.push(state, action, reward, done, value)\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps_done += 1\n",
    "\n",
    "                if self.env.portfolio.net_worth < self.env.portfolio.initial_net_worth * 0.3:\n",
    "                    done = True\n",
    "                    continue\n",
    "\n",
    "                if len(memory) < batch_size:\n",
    "                    continue\n",
    "\n",
    "                if True or steps_done % batch_size == 0:\n",
    "                    self._apply_gradient_descent(memory,\n",
    "                                             batch_size,\n",
    "                                             learning_rate,\n",
    "                                             discount_factor,\n",
    "                                             entropy_c)\n",
    "\n",
    "                if n_steps and steps_done >= n_steps:\n",
    "                    done = True\n",
    "                    #stop_training = True\n",
    "\n",
    "            is_checkpoint = save_every and episode % save_every == 0\n",
    "\n",
    "            if save_path and (is_checkpoint or episode == n_episodes):\n",
    "                self.save(save_path, episode=episode)\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "        mean_reward = total_reward / steps_done\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "\n",
    "\n",
    "agent = A2C_LSTMAgent_DEV(env)\n",
    "\n",
    "agent.train(n_steps=data.shape[0], batch_size=24*7, n_episodes=500, save_path=\"D:/Users/suuser/Desktop\")\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}